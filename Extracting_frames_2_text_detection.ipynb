{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def remove_horizontal_lines(image):\n",
    "    # Convert the image to true black and white from grayscale\n",
    "    threshold, image_bin = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    # Invert the image to change white to black and vice versa\n",
    "    image_inv = 255 - image_bin\n",
    "\n",
    "    # Define kernels for horizontal lines\n",
    "    kernel_len = np.array(image).shape[1] // 100\n",
    "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_len, 20))\n",
    "\n",
    "    # Remove anything that is not a horizontal line\n",
    "    image_inv = cv2.erode(image_inv, horizontal_kernel, iterations=3)\n",
    "    horizontal_lines = cv2.dilate(image_inv, horizontal_kernel, iterations=5)\n",
    "\n",
    "    # Subtract horizontal lines from the original image to remove them\n",
    "    image_without_horizontal_lines = cv2.subtract(255 * np.ones_like(image), horizontal_lines)\n",
    "\n",
    "    return image_without_horizontal_lines\n",
    "\n",
    "# Path to the recorded video file\n",
    "video_path = \"C:\\\\Users\\\\User\\\\Downloads\\\\TheAware.AI\\\\mp4_converted_video_journal.mp4\"\n",
    "\n",
    "# Create an \"output\" folder if it doesn't exist\n",
    "output_folder = \"output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize variables to store the previous bounding box position\n",
    "prev_x, prev_y, prev_w, prev_h = 50, 50, 50, 50\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Finished\")\n",
    "        break\n",
    "\n",
    "    # Apply Sobel operator for vertical gradient\n",
    "    sobel_y = cv2.Sobel(frame, cv2.CV_64F, 0, 1, ksize=1)\n",
    "    sobel_x = cv2.Sobel(frame, cv2.CV_64F, 1, 0, ksize=1)\n",
    "    sobel_y = np.abs(sobel_y)\n",
    "    sobel_x = np.abs(sobel_x)\n",
    "    sobel_y = np.uint8(sobel_y)\n",
    "    sobel_x = np.uint8(sobel_x)\n",
    "    sobel = sobel_x + sobel_y\n",
    "\n",
    "    # Apply Canny edge detection to the vertical gradient\n",
    "    edges = cv2.Canny(sobel, 90, 150)\n",
    "\n",
    "    # Remove horizontal lines\n",
    "    new_image = remove_horizontal_lines(edges)\n",
    "    # Define a rectangular kernel (you can adjust the size)\n",
    "    #kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "    # Perform erosion\n",
    "    #erosion_result = cv2.erode(new_image, kernel, iterations=1)\n",
    "    # Find contours in the binary image\n",
    "    contours, _ = cv2.findContours(new_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Sort contours based on their areas in descending order\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "    # Draw a single rectangle around the largest contour that follows a certain area threshold\n",
    "    if contours:\n",
    "        max_contour = contours[0]\n",
    "        if cv2.contourArea(max_contour) > 20000:\n",
    "            x, y, w, h = cv2.boundingRect(max_contour)\n",
    "\n",
    "            # Stabilize the bounding box by using the previous position\n",
    "            x = int(0.9 * prev_x + 0.1 * x)\n",
    "            y = int(0.9 * prev_y + 0.1 * y)\n",
    "            w = int(0.9 * prev_w + 0.1 * w)\n",
    "            h = int(0.9 * prev_h + 0.1 * h)\n",
    "            # Save the processed frame with a unique filename\n",
    "            frame_count += 1\n",
    "            filename = os.path.join(output_folder, f\"frame_{frame_count}.jpg\")\n",
    "            # cv2.imwrite(filename, frame)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            # Add text \"Text Region\" on the bounding box\n",
    "            cv2.putText(frame, \"Text Region\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "\n",
    "            # Update the previous position\n",
    "            prev_x, prev_y, prev_w, prev_h = x, y, w, h\n",
    "\n",
    "    # Display the processed frame\n",
    "    cv2.imshow(\"Processed Frame\", frame)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "# Input and output folders\n",
    "input_folder = \"C:\\\\Users\\\\User\\\\Downloads\\\\TheAware.AI\\\\output\"\n",
    "output_folder = \"C:\\\\Users\\\\User\\\\Downloads\\\\TheAware.AI\\\\output_final\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Read all frames from the input folder and sort them based on filenames\n",
    "frames = sorted([f for f in os.listdir(input_folder) if f.endswith('.jpg')],\n",
    "                key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "# Select the 5th, 15th, 25th, and 30th frames for every 30 frames\n",
    "selected_frames_indices = [4, 14, 24, 29]  # 0-based indices\n",
    "\n",
    "# Save the selected frames for every 30 frames\n",
    "for i in range(0, len(frames), 30):\n",
    "    for selected_frame_idx in selected_frames_indices:\n",
    "        current_frame_idx = i + selected_frame_idx\n",
    "        if current_frame_idx < len(frames):\n",
    "            selected_frame = cv2.imread(os.path.join(input_folder, frames[current_frame_idx]))\n",
    "            cv2.imwrite(os.path.join(output_folder, f\"selected_frame_{current_frame_idx + 1}.jpg\"), selected_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_keyframes_sift(directory, output_directory, frame_interval=10, ratio_threshold=0.7):\n",
    "    # Create output directory if not exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # List to store selected keyframes\n",
    "    selected_keyframes = []\n",
    "\n",
    "    # Iterate through the frames in the directory\n",
    "    frames = sorted([f for f in os.listdir(directory) if f.endswith('.jpg')],\n",
    "                    key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "    for i in range(0, len(frames), frame_interval):\n",
    "        filename = frames[i]\n",
    "        current_frame_path = os.path.join(directory, filename)\n",
    "\n",
    "        # Read the current frame\n",
    "        current_frame = cv2.imread(current_frame_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Initialize SIFT detector\n",
    "        sift = cv2.SIFT_create()\n",
    "\n",
    "        # Detect keypoints and descriptors\n",
    "        kp1, des1 = sift.detectAndCompute(current_frame, None)\n",
    "\n",
    "        # Skip if no keypoints found\n",
    "        if len(kp1) == 0:\n",
    "            continue\n",
    "\n",
    "        # Flag to check if the frame has matches with other selected keyframes\n",
    "        has_matches = False\n",
    "\n",
    "        # Compare with selected keyframes\n",
    "        for keyframe in selected_keyframes:\n",
    "            kp2, des2 = sift.detectAndCompute(keyframe, None)\n",
    "\n",
    "            # Use the Brute-Force Matcher with KNN\n",
    "            bf = cv2.BFMatcher()\n",
    "            matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "            # Apply ratio test\n",
    "            good_matches = [m for m, n in matches if m.distance < ratio_threshold * n.distance]\n",
    "\n",
    "            # If matches are found, set the flag and break\n",
    "            if len(good_matches) > 0.2 * len(kp1):\n",
    "                has_matches = True\n",
    "                break\n",
    "\n",
    "        # If no matches found, consider the frame as a keyframe\n",
    "        if not has_matches:\n",
    "            # Save the keyframe\n",
    "            output_path = os.path.join(output_directory, f'KeyFrame_{filename}')\n",
    "            cv2.imwrite(output_path, current_frame)\n",
    "\n",
    "            # Add the keyframe to the selected keyframes list\n",
    "            selected_keyframes.append(current_frame)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = 'C:\\\\Users\\\\User\\\\Downloads\\\\TheAware.AI\\\\output_final'\n",
    "    output_directory = 'C:\\\\Users\\\\User\\\\Downloads\\\\TheAware.AI\\\\keyframes_sift_output_final_n'\n",
    "\n",
    "    extract_keyframes_sift(input_directory, output_directory, frame_interval=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Function to calculate structural similarity index between two images\n",
    "def calculate_ssim(image1, image2):\n",
    "    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "    return ssim(gray1, gray2)\n",
    "\n",
    "# Function to select key frames based on structural similarity\n",
    "def select_key_frames(input_directory, output_directory, threshold=0.5):\n",
    "    # Create the output directory for key frames\n",
    "    keyframes_directory = os.path.join(output_directory, 'keyframes_ssim_new')\n",
    "    os.makedirs(keyframes_directory, exist_ok=True)\n",
    "\n",
    "    image_files = sorted([f for f in os.listdir(input_directory) if f.endswith('.jpg')],\n",
    "                     key=lambda x: int(re.search(r'\\d+', os.path.splitext(x)[0]).group()))\n",
    "\n",
    "    # Initialize variables to track key frames\n",
    "    key_frames = [cv2.imread(os.path.join(input_directory, image_files[0]))]\n",
    "    prev_frame = key_frames[0]\n",
    "\n",
    "    # Iterate through the rest of the frames\n",
    "    for image_file in image_files[1:]:\n",
    "        current_frame = cv2.imread(os.path.join(input_directory, image_file))\n",
    "\n",
    "        # Calculate structural similarity index\n",
    "        similarity_index = calculate_ssim(prev_frame, current_frame)\n",
    "\n",
    "        # If the similarity index is below the threshold, consider it a key frame\n",
    "        if similarity_index < threshold:\n",
    "            key_frames.append(current_frame)\n",
    "            prev_frame = current_frame\n",
    "\n",
    "    # Save the key frames to the new directory\n",
    "    for idx, key_frame in enumerate(key_frames):\n",
    "        output_path = os.path.join(keyframes_directory, f'KeyFrame_{idx + 1}.jpg')\n",
    "        cv2.imwrite(output_path, key_frame)\n",
    "\n",
    "# Specify the input and output directories\n",
    "input_directory = 'C:\\\\Users\\\\User\\\\Downloads\\\\TheAware.AI\\\\keyframes_sift_output_final_n'\n",
    "output_directory = 'C:\\\\Users\\\\User\\\\Downloads\\\\TheAware.AI\\\\final_frames_ssim'\n",
    "\n",
    "# Select key frames based on structural similarity\n",
    "select_key_frames(input_directory, output_directory, threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import vision\n",
    "import re\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'C:\\\\Users\\\\User\\\\Downloads\\\\TheAware.AI\\\\mirror-theaware-ai-07817b987170.json'\n",
    "\n",
    "vision_client = vision.ImageAnnotatorClient()\n",
    "image = vision.Image()\n",
    "\n",
    "# Specify the local file path of the image\n",
    "IMAGE_PATH = 'C:\\\\Users\\\\User\\\\Downloads\\\\TheAware.AI\\\\final_frames_ssim\\\\keyframes_ssim_new\\\\KeyFrame_2.jpg'\n",
    "\n",
    "# Read the image file and set it as the content of the 'image' object\n",
    "with open(IMAGE_PATH, 'rb') as image_file:\n",
    "    image_content = image_file.read()\n",
    "    image.content = image_content\n",
    "\n",
    "# Perform text detection on the image\n",
    "response = vision_client.text_detection(image=image)\n",
    "\n",
    "# Extract text annotations from the response\n",
    "text_annotations = response.text_annotations\n",
    "\n",
    "# Process the detected text as needed\n",
    "if text_annotations:\n",
    "    # Assuming you want the description of the first text annotation\n",
    "    text = text_annotations[0].description\n",
    "\n",
    "else:\n",
    "    print(\"No text detected in the image.\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import vision\n",
    "import re\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'C:\\\\Users\\\\User\\\\Downloads\\\\TheAware.AI\\\\mirror-theaware-ai-07817b987170.json'\n",
    "\n",
    "vision_client = vision.ImageAnnotatorClient()\n",
    "image = vision.Image()\n",
    "\n",
    "# Specify the local file path of the image\n",
    "IMAGE_PATH = 'C:\\\\Users\\\\User\\\\Downloads\\\\TheAware.AI\\\\animaldoodles-1.jpg'\n",
    "\n",
    "# Read the image file and set it as the content of the 'image' object\n",
    "with open(IMAGE_PATH, 'rb') as image_file:\n",
    "    image_content = image_file.read()\n",
    "    image.content = image_content\n",
    "\n",
    "    # Perform object detection\n",
    "response = vision_client.object_localization(image=image)\n",
    "objects = response.localized_object_annotations\n",
    "\n",
    "    # Print the results\n",
    "for obj in objects:\n",
    "        print(f\"Object name: {obj.name}\")\n",
    "        print(f\"Confidence: {obj.score:.2%}\")\n",
    "        print(f\"Bounding box vertices:\")\n",
    "        for vertex in obj.bounding_poly.normalized_vertices:\n",
    "            print(f\"  - ({vertex.x}, {vertex.y})\")\n",
    "\n",
    "# Process the detected text as needed\n",
    "#if text_annotations:\n",
    "    # Assuming you want the description of the first text annotation\n",
    "#    text = text_annotations[0].description\n",
    "\n",
    "#else:\n",
    "#    print(\"No text detected in the image.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document AI for Text Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence\n",
    "\n",
    "from google.api_core.client_options import ClientOptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade google-cloud-documentai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "def convert_text_to_pdf(text, output_pdf):\n",
    "    # Create a PDF file\n",
    "    with open(output_pdf, 'wb') as pdf_file:\n",
    "        # Create a PDF canvas\n",
    "        pdf = canvas.Canvas(pdf_file, pagesize=letter)\n",
    "\n",
    "        # Add the extracted text to the PDF\n",
    "        pdf.drawString(100, 700, text)\n",
    "\n",
    "        # Save the PDF file\n",
    "        pdf.save()\n",
    "\n",
    "    print(f\"Text converted to PDF successfully: {output_pdf}\")\n",
    "\n",
    "# Specify the output PDF file for the converted text\n",
    "output_text_pdf = 'C:\\\\Users\\\\User\\\\Downloads\\\\TheAware.AI\\\\Digital_text.pdf'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_ocr_sample(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    processor_version: str,\n",
    "    file_path: str,\n",
    "    mime_type: str,\n",
    ") -> None:\n",
    "    # Optional: Additional configurations for Document OCR Processor.\n",
    "    # For more information: https://cloud.google.com/document-ai/docs/document-ocr\n",
    "    process_options = documentai.ProcessOptions(\n",
    "        ocr_config=documentai.OcrConfig(\n",
    "            compute_style_info=True,\n",
    "            enable_native_pdf_parsing=True,\n",
    "            enable_image_quality_scores=True,\n",
    "            enable_symbol=True,\n",
    "        )\n",
    "    )\n",
    "    # Online processing request to Document AI\n",
    "    document = process_document(\n",
    "        project_id,\n",
    "        location,\n",
    "        processor_id,\n",
    "        processor_version,\n",
    "        file_path,\n",
    "        mime_type,\n",
    "        process_options=process_options,\n",
    "    )\n",
    "\n",
    "    text = document.text\n",
    "\n",
    "    def convert_text_to_pdf(lines, output_pdf):\n",
    "        # Create a PDF file\n",
    "        with open(output_pdf, 'wb') as pdf_file:\n",
    "            # Create a PDF canvas\n",
    "            pdf = canvas.Canvas(pdf_file, pagesize=letter)\n",
    "\n",
    "            # Add each line of text to the PDF\n",
    "            y_position = 700  # Adjust the starting Y position as needed\n",
    "            for line in lines:\n",
    "                pdf.drawString(100, y_position, line)\n",
    "                y_position -= 12  # Adjust the line spacing as needed\n",
    "\n",
    "            # Save the PDF file\n",
    "            pdf.save()\n",
    "        print(f\"Text converted to PDF successfully: {output_pdf}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Assuming 'text' is the extracted text from the Document AI OCR process, split it into lines\n",
    "    text_lines = text.split('\\n')\n",
    "\n",
    "    # Specify the output PDF file for the converted text\n",
    "    output_text_pdf = 'C:\\\\Users\\\\User\\\\Downloads\\\\TheAware.AI\\\\Digital_text_2.pdf'\n",
    "    \n",
    "    # Call the function to convert text to PDF\n",
    "    #convert_text_to_pdf(text_lines, output_text_pdf)\n",
    "    print(f\"Full document text: {text}\\n\")\n",
    "    print(f\"There are {len(document.pages)} page(s) in this document.\\n\")\n",
    "    \n",
    "    for page in document.pages:\n",
    "        print(f\"Page {page.page_number}:\")\n",
    "        print_page_dimensions(page.dimension)\n",
    "        print_detected_langauges(page.detected_languages)\n",
    "\n",
    "        print_blocks(page.blocks, text)\n",
    "        print_paragraphs(page.paragraphs, text)\n",
    "        print_lines(page.lines, text)\n",
    "        print_tokens(page.tokens, text)\n",
    "\n",
    "        if page.symbols:\n",
    "            print_symbols(page.symbols, text)\n",
    "\n",
    "        if page.image_quality_scores:\n",
    "            print_image_quality_scores(page.image_quality_scores)\n",
    "\n",
    "    if document.text_styles:\n",
    "        print_styles(document.text_styles, text)\n",
    "\n",
    "\n",
    "def print_page_dimensions(dimension: documentai.Document.Page.Dimension) -> None:\n",
    "    print(f\"    Width: {str(dimension.width)}\")\n",
    "    print(f\"    Height: {str(dimension.height)}\")\n",
    "\n",
    "\n",
    "def print_detected_langauges(\n",
    "    detected_languages: Sequence[documentai.Document.Page.DetectedLanguage],\n",
    ") -> None:\n",
    "    print(\"    Detected languages:\")\n",
    "    for lang in detected_languages:\n",
    "        print(f\"        {lang.language_code} ({lang.confidence:.1%} confidence)\")\n",
    "\n",
    "\n",
    "def print_blocks(blocks: Sequence[documentai.Document.Page.Block], text: str) -> None:\n",
    "    print(f\"    {len(blocks)} blocks detected:\")\n",
    "    first_block_text = layout_to_text(blocks[0].layout, text)\n",
    "    print(f\"        First text block: {repr(first_block_text)}\")\n",
    "    last_block_text = layout_to_text(blocks[-1].layout, text)\n",
    "    print(f\"        Last text block: {repr(last_block_text)}\")\n",
    "\n",
    "\n",
    "def print_paragraphs(\n",
    "    paragraphs: Sequence[documentai.Document.Page.Paragraph], text: str\n",
    ") -> None:\n",
    "    print(f\"    {len(paragraphs)} paragraphs detected:\")\n",
    "    first_paragraph_text = layout_to_text(paragraphs[0].layout, text)\n",
    "    print(f\"        First paragraph text: {repr(first_paragraph_text)}\")\n",
    "    last_paragraph_text = layout_to_text(paragraphs[-1].layout, text)\n",
    "    print(f\"        Last paragraph text: {repr(last_paragraph_text)}\")\n",
    "\n",
    "\n",
    "def print_lines(lines: Sequence[documentai.Document.Page.Line], text: str) -> None:\n",
    "    print(f\"    {len(lines)} lines detected:\")\n",
    "    first_line_text = layout_to_text(lines[0].layout, text)\n",
    "    print(f\"        First line text: {repr(first_line_text)}\")\n",
    "    last_line_text = layout_to_text(lines[-1].layout, text)\n",
    "    print(f\"        Last line text: {repr(last_line_text)}\")\n",
    "\n",
    "\n",
    "def print_tokens(tokens: Sequence[documentai.Document.Page.Token], text: str) -> None:\n",
    "    print(f\"    {len(tokens)} tokens detected:\")\n",
    "    first_token_text = layout_to_text(tokens[0].layout, text)\n",
    "    first_token_break_type = tokens[0].detected_break.type_.name\n",
    "    print(f\"        First token text: {repr(first_token_text)}\")\n",
    "    print(f\"        First token break type: {repr(first_token_break_type)}\")\n",
    "    last_token_text = layout_to_text(tokens[-1].layout, text)\n",
    "    last_token_break_type = tokens[-1].detected_break.type_.name\n",
    "    print(f\"        Last token text: {repr(last_token_text)}\")\n",
    "    print(f\"        Last token break type: {repr(last_token_break_type)}\")\n",
    "\n",
    "\n",
    "def print_symbols(\n",
    "    symbols: Sequence[documentai.Document.Page.Symbol], text: str\n",
    ") -> None:\n",
    "    print(f\"    {len(symbols)} symbols detected:\")\n",
    "    first_symbol_text = layout_to_text(symbols[0].layout, text)\n",
    "    print(f\"        First symbol text: {repr(first_symbol_text)}\")\n",
    "    last_symbol_text = layout_to_text(symbols[-1].layout, text)\n",
    "    print(f\"        Last symbol text: {repr(last_symbol_text)}\")\n",
    "\n",
    "\n",
    "def print_image_quality_scores(\n",
    "    image_quality_scores: documentai.Document.Page.ImageQualityScores,\n",
    ") -> None:\n",
    "    print(f\"    Quality score: {image_quality_scores.quality_score:.1%}\")\n",
    "    print(\"    Detected defects:\")\n",
    "\n",
    "    for detected_defect in image_quality_scores.detected_defects:\n",
    "        print(f\"        {detected_defect.type_}: {detected_defect.confidence:.1%}\")\n",
    "\n",
    "\n",
    "def print_styles(styles: Sequence[documentai.Document.Style], text: str) -> None:\n",
    "    print(f\"    {len(styles)} styles detected:\")\n",
    "    first_style_text = layout_to_text(styles[0].layout, text)\n",
    "    print(f\"        First style text: {repr(first_style_text)}\")\n",
    "    print(f\"           Color: {styles[0].color}\")\n",
    "    print(f\"           Background Color: {styles[0].background_color}\")\n",
    "    print(f\"           Font Weight: {styles[0].font_weight}\")\n",
    "    print(f\"           Text Style: {styles[0].text_style}\")\n",
    "    print(f\"           Text Decoration: {styles[0].text_decoration}\")\n",
    "    print(f\"           Font Size: {styles[0].font_size.size}{styles[0].font_size.unit}\")\n",
    "    print(f\"           Font Family: {styles[0].font_family}\")\n",
    "\n",
    "\n",
    "def process_document(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    processor_version: str,\n",
    "    file_path: str,\n",
    "    mime_type: str,\n",
    "    process_options: Optional[documentai.ProcessOptions] = None,\n",
    ") -> documentai.Document:\n",
    "    # You must set the `api_endpoint` if you use a location other than \"us\".\n",
    "    client = documentai.DocumentProcessorServiceClient(\n",
    "        client_options=ClientOptions(\n",
    "            api_endpoint=f\"{location}-documentai.googleapis.com\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # The full resource name of the processor version, e.g.:\n",
    "    # `projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}`\n",
    "    # You must create a processor before running this sample.\n",
    "    name = client.processor_version_path(\n",
    "        project_id, location, processor_id, processor_version\n",
    "    )\n",
    "\n",
    "    # Read the file into memory\n",
    "    with open(file_path, \"rb\") as image:\n",
    "        image_content = image.read()\n",
    "\n",
    "    # Configure the process request\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=name,\n",
    "        raw_document=documentai.RawDocument(content=image_content, mime_type=mime_type),\n",
    "        # Only supported for Document OCR processor\n",
    "        process_options=process_options,\n",
    "    )\n",
    "\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    # For a full list of `Document` object attributes, reference this page:\n",
    "    # https://cloud.google.com/document-ai/docs/reference/rest/v1/Document\n",
    "    return result.document\n",
    "\n",
    "\n",
    "def layout_to_text(layout: documentai.Document.Page.Layout, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Document AI identifies text in different parts of the document by their\n",
    "    offsets in the entirety of the document\"s text. This function converts\n",
    "    offsets to a string.\n",
    "    \"\"\"\n",
    "    # If a text segment spans several lines, it will\n",
    "    # be stored in different text segments.\n",
    "    return \"\".join(\n",
    "        text[int(segment.start_index) : int(segment.end_index)]\n",
    "        for segment in layout.text_anchor.text_segments\n",
    "    )\n",
    "\n",
    "# TODO(developer): Edit these variables before running the sample.\n",
    "project_id = \"theaware-ai\"\n",
    "location = \"us\"  # Format is 'us' or 'eu'\n",
    "processor_id = \"7\"  # Create processor before running sample\n",
    "processor_version = \"pretrained-ocr-v2.0-2023-06-02\"\n",
    "file_path = \"C:\\\\Users\\\\User\\\\Downloads\\\\TheAware.AI\\\\final_frames_ssim\\\\keyframes_ssim_new\\\\KeyFrame_10.jpg\"\n",
    "mime_type = \"image/jpeg\"  # Refer to https://cloud.google.com/document-ai/docs/file-types for supported file types\n",
    "\n",
    "\n",
    "process_document_ocr_sample(\n",
    "    project_id=project_id,\n",
    "    location=location,\n",
    "    processor_id=processor_id,\n",
    "    processor_version=processor_version,\n",
    "    file_path=file_path,\n",
    "    mime_type=mime_type,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
